{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Question Pairs Classification with GPT-2 and PEFT\n",
    "\n",
    "This notebook implements a medical question similarity classification model using GPT-2 and Parameter-Efficient Fine-Tuning (PEFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global configurations\n",
    "SEED = 42\n",
    "MODEL_NAME = \"gpt2\"\n",
    "ID2LABEL = {0: \"not_similar\", 1: \"similar\"}\n",
    "LABEL2ID = {\"not_similar\": 0, \"similar\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Seed Setting and Weights & Biases Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds():\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.mps.is_available():\n",
    "        torch.mps.manual_seed(SEED)\n",
    "    set_seed(SEED)\n",
    "\n",
    "def setup_wandb():\n",
    "    os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "    config = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 8,\n",
    "        \"lora_r\": 8,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"seed\": SEED,\n",
    "    }\n",
    "    wandb.init(\n",
    "        project=\"medical-qa-peft\",\n",
    "        name=\"gpt2-lora-experiment\",\n",
    "        config=config,\n",
    "        settings=wandb.Settings(console=\"off\"),\n",
    "    )\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_dataset():\n",
    "    dataset = load_dataset(\"medical_questions_pairs\")\n",
    "    if \"validation\" not in dataset:\n",
    "        train_testvalid = dataset[\"train\"].train_test_split(test_size=0.2, seed=SEED)\n",
    "        test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=SEED)\n",
    "        dataset = {\n",
    "            \"train\": train_testvalid[\"train\"],\n",
    "            \"validation\": test_valid[\"train\"],\n",
    "            \"test\": test_valid[\"test\"],\n",
    "        }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Computation and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"binary\"\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    metrics = {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "    wandb.log(metrics)\n",
    "    return metrics\n",
    "\n",
    "def get_training_args(config):\n",
    "    return TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"batch_size\"],\n",
    "        num_train_epochs=config[\"epochs\"],\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, tokenizer):\n",
    "    def preprocess_function(examples):\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"question_1\"],\n",
    "            examples[\"question_2\"],\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        tokenized[\"labels\"] = examples[\"label\"]\n",
    "        return tokenized\n",
    "\n",
    "    return {\n",
    "        split: dataset[split].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset[split].column_names,\n",
    "        )\n",
    "        for split in dataset.keys()\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, trainer, model_name=\"\"):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"{model_name} metrics:\", metrics)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training and Evaluation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtim_lin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.9s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tim/Devs/Projects/gpt2-classification-peft/wandb/run-20250324_223626-v2onfqrk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tim_lin/medical-qa-peft/runs/v2onfqrk' target=\"_blank\">gpt2-lora-experiment</a></strong> to <a href='https://wandb.ai/tim_lin/medical-qa-peft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tim_lin/medical-qa-peft' target=\"_blank\">https://wandb.ai/tim_lin/medical-qa-peft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tim_lin/medical-qa-peft/runs/v2onfqrk' target=\"_blank\">https://wandb.ai/tim_lin/medical-qa-peft/runs/v2onfqrk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07f94cdd8894279944f01d62c579a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145a3ba82caa4f3a800ad7d229558bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8c79903c124e39953c559b30ea8ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize settings\n",
    "set_seeds()\n",
    "config = setup_wandb()\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Set up tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Preprocess data\n",
    "encoded_dataset = preprocess_data(dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model Setup and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Base Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tim/miniforge3/envs/gen-ai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model metrics: {'eval_loss': 7.587254047393799, 'eval_model_preparation_time': 0.0009, 'eval_accuracy': 0.45901639344262296, 'eval_f1': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 2.019, 'eval_samples_per_second': 151.066, 'eval_steps_per_second': 19.317}\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID,\n",
    ")\n",
    "\n",
    "# Get training arguments\n",
    "training_args = get_training_args(wandb.config)\n",
    "\n",
    "# Create trainer for base model evaluation\n",
    "base_trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate base model\n",
    "base_metrics = evaluate_model(base_model, base_trainer, \"Base Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 812,544 || all params: 125,253,888 || trainable%: 0.6487\n",
      "Trainable parameters: None\n",
      "Training PEFT model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [915/915 02:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.766343</td>\n",
       "      <td>0.593443</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.675214</td>\n",
       "      <td>0.478788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.719700</td>\n",
       "      <td>0.677142</td>\n",
       "      <td>0.629508</td>\n",
       "      <td>0.563707</td>\n",
       "      <td>0.776596</td>\n",
       "      <td>0.442424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.667900</td>\n",
       "      <td>0.658817</td>\n",
       "      <td>0.636066</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.448485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=915, training_loss=1.289413628812696, metrics={'train_runtime': 135.3262, 'train_samples_per_second': 54.047, 'train_steps_per_second': 6.761, 'total_flos': 482345291612160.0, 'train_loss': 1.289413628812696, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=config[\"lora_r\"],\n",
    "    lora_alpha=config[\"lora_alpha\"],\n",
    "    lora_dropout=config[\"lora_dropout\"],\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    fan_in_fan_out=True,\n",
    ")\n",
    "\n",
    "# Create PEFT model\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "print(\"Trainable parameters:\", peft_model.print_trainable_parameters())\n",
    "\n",
    "# Create trainer for PEFT model\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train PEFT model\n",
    "print(\"Training PEFT model...\")\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating PEFT Model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT Model metrics: {'eval_loss': 0.6588172912597656, 'eval_accuracy': 0.6360655737704918, 'eval_f1': 0.5714285714285714, 'eval_precision': 0.7872340425531915, 'eval_recall': 0.4484848484848485, 'eval_runtime': 2.0476, 'eval_samples_per_second': 148.957, 'eval_steps_per_second': 19.047, 'epoch': 3.0}\n",
      "\n",
      "Performance Comparison:\n",
      "Base Model Accuracy: 0.4590\n",
      "PEFT Model Accuracy: 0.6361\n",
      "Improvement: 17.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate PEFT model\n",
    "peft_metrics = evaluate_model(peft_model, peft_trainer, \"PEFT Model\")\n",
    "\n",
    "# Compare and record performance differences\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"Base Model Accuracy: {base_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"PEFT Model Accuracy: {peft_metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Improvement: {(peft_metrics['eval_accuracy'] - base_metrics['eval_accuracy'])*100:.2f}%\")\n",
    "\n",
    "# Log comparison results to wandb\n",
    "comparison_data = [\n",
    "    [\"Base Model\", base_metrics[\"eval_accuracy\"]],\n",
    "    [\"PEFT Model\", peft_metrics[\"eval_accuracy\"]]\n",
    "]\n",
    "wandb.log({\n",
    "    \"model_comparison\": wandb.plot.bar(\n",
    "        wandb.Table(data=comparison_data, columns=[\"Model Type\", \"Accuracy\"]),\n",
    "        \"Model Type\",\n",
    "        \"Accuracy\",\n",
    "        title=\"Model Accuracy Comparison\"\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model and Generate Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test results saved to predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▆███▇</td></tr><tr><td>eval/accuracy</td><td>▁▆███</td></tr><tr><td>eval/f1</td><td>▁████</td></tr><tr><td>eval/loss</td><td>█▁▁▁▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁</td></tr><tr><td>eval/precision</td><td>▁▇███</td></tr><tr><td>eval/recall</td><td>▁█▇██</td></tr><tr><td>eval/runtime</td><td>▇▆▅▁█</td></tr><tr><td>eval/samples_per_second</td><td>▂▃▄█▁</td></tr><tr><td>eval/steps_per_second</td><td>▂▃▄█▁</td></tr><tr><td>f1</td><td>▁████▇</td></tr><tr><td>precision</td><td>▁▇███▇</td></tr><tr><td>recall</td><td>▁█▇██▇</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/precision</td><td>▁</td></tr><tr><td>test/recall</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▆▃▅▆▃▂▂▂▂▂▁▃▁▂▁▂▁▂▂▁▁▂▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>█████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>███▆▇▅▄▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.61967</td></tr><tr><td>eval/accuracy</td><td>0.63607</td></tr><tr><td>eval/f1</td><td>0.57143</td></tr><tr><td>eval/loss</td><td>0.65882</td></tr><tr><td>eval/model_preparation_time</td><td>0.0009</td></tr><tr><td>eval/precision</td><td>0.78723</td></tr><tr><td>eval/recall</td><td>0.44848</td></tr><tr><td>eval/runtime</td><td>2.0476</td></tr><tr><td>eval/samples_per_second</td><td>148.957</td></tr><tr><td>eval/steps_per_second</td><td>19.047</td></tr><tr><td>f1</td><td>0.52459</td></tr><tr><td>precision</td><td>0.68817</td></tr><tr><td>recall</td><td>0.42384</td></tr><tr><td>test/accuracy</td><td>0.61967</td></tr><tr><td>test/f1</td><td>0.52459</td></tr><tr><td>test/loss</td><td>0.67708</td></tr><tr><td>test/precision</td><td>0.68817</td></tr><tr><td>test/recall</td><td>0.42384</td></tr><tr><td>test/runtime</td><td>2.0585</td></tr><tr><td>test/samples_per_second</td><td>148.166</td></tr><tr><td>test/steps_per_second</td><td>18.946</td></tr><tr><td>total_flos</td><td>482345291612160.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>915</td></tr><tr><td>train/grad_norm</td><td>4.39668</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6679</td></tr><tr><td>train_loss</td><td>1.28941</td></tr><tr><td>train_runtime</td><td>135.3262</td></tr><tr><td>train_samples_per_second</td><td>54.047</td></tr><tr><td>train_steps_per_second</td><td>6.761</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gpt2-lora-experiment</strong> at: <a href='https://wandb.ai/tim_lin/medical-qa-peft/runs/v2onfqrk' target=\"_blank\">https://wandb.ai/tim_lin/medical-qa-peft/runs/v2onfqrk</a><br> View project at: <a href='https://wandb.ai/tim_lin/medical-qa-peft' target=\"_blank\">https://wandb.ai/tim_lin/medical-qa-peft</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250324_223626-v2onfqrk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save model\n",
    "peft_model.save_pretrained(\"./peft_model\")\n",
    "\n",
    "# Test set prediction and results saving\n",
    "test_results = peft_trainer.predict(encoded_dataset[\"test\"])\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"question_1\": [item[\"question_1\"] for item in dataset[\"test\"]],\n",
    "    \"question_2\": [item[\"question_2\"] for item in dataset[\"test\"]],\n",
    "    \"predictions\": test_results.predictions.argmax(axis=1),\n",
    "    \"true_labels\": test_results.label_ids,\n",
    "})\n",
    "\n",
    "test_df[\"prediction_text\"] = test_df[\"predictions\"].map(ID2LABEL)\n",
    "test_df[\"true_label_text\"] = test_df[\"true_labels\"].map(ID2LABEL)\n",
    "test_df[\"is_correct\"] = test_df[\"predictions\"] == test_df[\"true_labels\"]\n",
    "\n",
    "os.makedirs(\"test_results\", exist_ok=True)\n",
    "test_df.to_csv(\"test_results/predictions.csv\", index=False)\n",
    "print(\"\\nTest results saved to predictions.csv\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
